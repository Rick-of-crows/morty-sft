#### 3H Score
模型|总分|帮助性<br>helpfulness|真实性<br>honest|无害性<br>harmness|
---|---|---|---|---|
满分|2250|600|1000|650|
CW-Llama-70b-204B-4k |2061.5|632.0|861.5|568.0|
CW-Llama-16k |1944.5|589.5|819.0|536.0|
CW-Llama-8k|1933.0|584.0|838.0|511.0|
CW-Llama-8k-707B|1932.5|588.5|839.0|505.0|
CW-Llama-4k|1928.5|598.0|810.5|520.0|
CW-Llama-16k-707B|1912.0|589.0|822.5|501.0|
CW-Llama-32k-707B|1887.0|585.0|790.0|512.0|
CW-Llama-32k-707B-16kdata|1832.0|565.0|792.0|475.0|



#### EMB + DocQA
EMB模型|模型|自建测试集（200）|
---|---|---|
cosent55|CW-Llama-16k-707B + concat |83.3%|
cosent55|CW-Llama-16k-707B|81.2%|
cosent55|CW-Llama-8k-707B|79.2%|
cosent55|CW-Llama-4k|72.2%|

#### Single-Document QA
|             Model      | NarrativeQA | Qasper | MultiFieldQA-en | MultiFieldQA-zh | Total| AVG | 
| ----------------- | :---------: | :----: | :-------------: | :-------------: |:-------------: |:-------------: |
| GPT-3.5-Turbo-16k | 23.6 | 43.3 | 52.3 | 61.2 | 180.4 | 45.1|
| ChatGLM2-6B-32k | 21.1 | 31.5 | 46.2 | 51.6 | 150.4 | 37.6| 
| CW-Llama-70b-204B-4k | 20.51 | 37.29 | 49.21 | 42.22 | 149.23 | 37.3| 
| CW-Llama-32k-707B-16kdata | 18.95 | 30.6 | 53.76 | 44.68 | 147.99 | 37.0 |
| CW-Llama-32k-707B-without-flan | 19.92 | 33.62 | 52.65 | 40.24 | 146.43 | 36.61 |
| CW-Llama-16k-707B | 18.16 | 32.94 | 47.97 | 47.24 | 146.31 | 36.58 |
| CW-Llama-16k | 16.34 | 34.53 | 47.14 | 47.33 | 145.34 | 36.3 |
| CW-Llama-32k-707B | 16.57 | 34.26 | 50.11 | 43.89 | 144.83 | 36.21 |
| CW-Llama-8k | 16.16 | 34.69 | 44.55 | 46.75 | 142.15 | 35.5 |
| CW-Llama-8k-707B | 18.31 | 32.4 | 43.39 | 42.26 | 136.36 | 34.09 |
| Vicuna-v1.5-7B-16k | 19.4 | 26.1 | 38.5 | 43.0 | 127 | 31.8 |
| CW-Llama-4k | 16.11 | 27.85 | 42.32 | 37.1 | 123.38 |30.8 |
| LongChat-v1.5-7B-32k | 16.9 | 27.7 | 41.4 | 29.1 | 115.1 | 28.8 |
| ChatGLM2-6B | 11.8 | 22.5 | 35.0 | 33.2 | 102.5 | 25.6 |
| XGen-7B-8k | 18.0 | 18.1 | 37.7 | 14.8 | 88.6 |  22.2|
| Llama2-7B-chat-4k | 18.7 | 19.2 | 36.8 | 11.9 | 86.6 | 21.7 |
| InternLM-7B-8k | 12.1 | 16.7 | 23.4 | 33.6 | 85.8| 21.5 |




#### Multi-Document QA
|          model         | HotpotQA | 2WikiMQA | Musique | DuReader (zh) |Total |AVG|
| ----------------- | :------: | :------: | :-----: | :-----------: |:-------------: |:-------------: |
| CW-Llama-70b-204B-4k | 42.21 | 46.43 | 24.39 | 33.57 | 146.6 | 36.65| 
| GPT-3.5-Turbo-16k | 51.6 | 37.7 | 26.9 | 28.7 | 144.9 | 36.2| 
| ChatGLM2-6B-32k | 45.1 | 34.0 | 21.9 | 37.6 | 138.6  | 34.7| 
| CW-Llama-16k | 41.78 | 37.81 | 22.07 | 29.95 | 131.61 | 32.9 |
| CW-Llama-32k-707B-16kdata | 39.18 | 38.11 | 19.14 | 32.97 |129.4 | 32.35|
| CW-Llama-32k-707B-without-flan | 38.89 | 36.88 | 20.67 | 31.49 | 127.93 | 31.98 |
| CW-Llama-8k | 38.16 | 39.66 | 17.73 | 25.09 | 120.64 | 30.16 |
| CW-Llama-32k-707B | 33.27 | 32.99 | 17.34 | 30.87 | 114.47 | 28.62 |
| CW-Llama-16k-707B | 26.18 | 31.87 | 13.12 | 30.24 | 101.41 | 25.35 |
| CW-Llama-8k-707B | 30.01 | 33.72 | 12.46 | 23.4 | 99.59 | 24.9 |
| CW-Llama-4k | 25.98 | 25.57 | 12.15 | 23.6 | 87.3 | 21.8|
| LongChat-v1.5-7B-32k | 31.5 | 20.6 | 9.7 | 19.5 | 81.3 | 20.3 | 
| Vicuna-v1.5-7B-16k | 25.3 | 20.8 | 9.8 | 19.3 | 75.2  | 18.8|
| Llama2-7B-chat-4k | 25.4 | 32.8 | 9.4 | 5.2 | 72.8 | 18.2|
| XGen-7B-8k | 29.7 | 21.1 | 10.3 | 11.0 | 72.1 | 18.0|
| InternLM-7B-8k | 28.7 | 22.8 | 9.0 | 11.1 | 71.6 | 17.9 |
| ChatGLM2-6B | 22.4 | 20.1 | 6.1 | 16.3 | 64.9 | 16.2|




#### Document QA
|          model         | Single-Document | Multi-Document |Total |AVG|
| ----------------- | :------: | :------: | :-----: | :-----: | 
| GPT-3.5-Turbo-16k | 180.4 | 144.9  | 325.3 |40.66 | 
| CW-Llama-70b-204B-4k | 149.23 | 146.6 | 295.83 | 36.98 |
| ChatGLM2-6B-32k | 150.4 | 138.6 | 289 | 36.13| 
| CW-Llama-32k-707B-16kdata | 147.99 | 129.4 | 277.39 | 34.67|
| CW-Llama-16k | 145.34 |131.61 | 276.95 | 34.62|
| CW-Llama-32k-707B-without-flan | 146.43 | 127.93  | 274.36 | 34.30|
| CW-Llama-8k | 142.15 |120.64 | 262.79 | 32.85|
| CW-Llama-32k-707B | 144.83 | 114.47 | 259.3 | 32.41|
| CW-Llama-16k-707B | 146.31 |101.41 | 247.72 | 30.97|
| CW-Llama-8k-707B | 136.36 | 99.59 | 235.95 | 29.49|
| CW-Llama-4k | 123.38 | 87.3 | 210.68 | 26.34|
| Vicuna-v1.5-7B-16k | 127 | 75.2 | 202.2 | 25.28|
| LongChat-v1.5-7B-32k | 115.1 | 81.3 | 196.4 | 24.55| 
| ChatGLM2-6B | 102.5 | 64.9 | 167.4 | 20.93|
| XGen-7B-8k | 88.6 | 72.1 | 160.7 | 20.09|
| Llama2-7B-chat-4k | 86.6 | 72.8 | 159.4 | 19.93| 
| InternLM-7B-8k | 85.8 | 71.6 | 157.4 | 19.68|


#### Summarization
|       model     | GovReport | QMSum | MultiNews | VCSUM (zh) |
|:-----------|:---------:|:-----:|:-----:|:-----:|
| GPT-3.5-Turbo-16k | 29.5 | 23.4 | 26.7 | 16.0 |
| Llama2-7B-chat-4k | 27.3 | 20.8 | 25.8 | 0.2 |
| LongChat-v1.5-7B-32k | 30.8 | 22.7 | 26.4 | 9.9 |
| XGen-7B-8k | 27.3 | 20.5 | 26.2 | 2.2 |
| InternLM-7B-8k | 9.7 | 15.9 | 22.8 | 12.4 |
| ChatGLM2-6B | 23.2 | 21.1 | 25.2 | 14.5 |
| ChatGLM2-6B-32k | 32.4 | 24.0 | 26.5 | 16.2 |
| Vicuna-v1.5-7B-16k | 27.9 | 22.8 | 27.2 | 15.1 |

#### Few-shot Learning
|   model  | TREC | TriviaQA | SAMSum | LSHT (zh) |
| --- | :-: | :-: | :-: | :-: |
| GPT-3.5-Turbo-16k | 68.0 | 91.4 | 41.7 | 29.2 |
| Llama2-7B-chat-4k | 61.5 | 77.8 | 40.7 | 19.8 |
| LongChat-v1.5-7B-32k | 63.5 | 82.3 | 34.2 | 23.2 |
| XGen-7B-8k | 65.5 | 77.8 | 25.3 | 20.5 |
| InternLM-7B-8k | 52.0 | 77.8 | 21.2 | 15.2 |
| ChatGLM2-6B | 44.5 | 70.6 | 29.5 | 20.8 |
| ChatGLM2-6B-32k | 62.5 | 78.7 | 36.3 | 27.7 |
| Vicuna-v1.5-7B-16k | 71.5 | 86.2 | 40.8 | 28.8 |

#### Synthetic Tasks
|  model   | Passage Count | PassageRetrieval-en | PassageRetrieval-zh |
| --- | :-: | :-: | :-: |
| GPT-3.5-Turbo-16k | 4.5 | 71.0 | 77.5 |
| Llama2-7B-chat-4k | 2.1 | 9.8 | 0.5 |
| LongChat-v1.5-7B-32k | 1.0 | 30.5 | 7.6 |
| XGen-7B-8k | 2.1 | 8.5 | 3.5 |
| InternLM-7B-8k | 3.0 | 6.0 | 0.9 |
| ChatGLM2-6B | 2.5 | 3.0 | 6.5 |
| ChatGLM2-6B-32k | 1.5 | 77.0 | 64.5 |
| Vicuna-v1.5-7B-16k | 6.5 | 4.5 | 5.0 |

#### Code Completion
|  model   | LCC | RepoBench-P |
| --- | :-: | :-: |
| GPT-3.5-Turbo-16k | 54.7 | 53.6 |
| Llama2-7B-chat-4k | 52.4 | 43.8 |
| LongChat-v1.5-7B-32k | 53.0 | 55.3 |
| XGen-7B-8k | 38.6 | 38.6 |
| InternLM-7B-8k | 44.1 | 28.8 |
| ChatGLM2-6B | 49.0 | 43.2 |
| ChatGLM2-6B-32k | 55.6 | 49.9 |
| Vicuna-v1.5-7B-16k | 51.0 | 43.5 |
